{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social 4 \n",
    "\n",
    "## Goal: Find correlations between how well students do in school and other factors. The data set we will be using is from portugese schools in the subject of math.\n",
    "\n",
    "### Group Members:  Christine Asai, Alder Futon, Ashley Francis, Brooke Schmidt, Izaan Shaikh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm                          # statsmodels logistic regression\n",
    "from sklearn.linear_model import LogisticRegression   # sklearn logistic regression\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our Original Dataframe \n",
    "#### Student Performance Data Set taken from two Portuguese Secondary Education Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_df = pd.read_csv('student-mat.csv',delimiter = ';')\n",
    "math_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(math_df['age'],math_df['absences'],alpha = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Function: data_scrubber\n",
    "####   This function replaces the string values with numerical values. \n",
    "    Input: Dataframe. A list of column names to turn from strings into intergers.\n",
    "    Output: Dataframe, where the columns in the 'names' argument are now intergers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scrubber(df,names):\n",
    "    for name in names:\n",
    "        values = df[name].unique()\n",
    "        df[name].replace(values,range(0,values.size),inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the Boolean no/yes Values with 0 and 1 Respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = math_df.copy()\n",
    "numerical_df.replace(['no','yes'], [0,1], inplace=True)  #Replace the boolean yes/no values.\n",
    "numerical_df = data_scrubber(numerical_df,['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Histograms of All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning 1-20 Grade Rankings into 0-1 Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df['G3'] = numerical_df['G3']/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numerical_df[['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob','Fjob','reason','guardian','traveltime','studytime','failures','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic','famrel','freetime','goout','Dalc','Walc','health','absences']]\n",
    "Y = numerical_df['G3']\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.75,test_size=0.25,random_state=1)\n",
    "logit_model = sm.Logit(y_train, sm.add_constant(x_train))\n",
    "result = logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Correlation Between Features\n",
    "#### Specifically Looking at Each Features Correlation with G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df=numerical_df.corr()\n",
    "correlation_df[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (25, 20)\n",
    "plt.title('Coorelations with G3')\n",
    "sns.heatmap(correlation_df, cmap = 'Blues', annot = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Failures* had the highest negative correlation with G3\n",
    "#### Below is a scatter plot to get more insight to this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(numerical_df[\"failures\"],numerical_df[\"G3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Mother Education* had the highest positive correlation with G3\n",
    "#### Below is a scatter plot to get more insight to this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(numerical_df[\"Medu\"],numerical_df[\"G3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Quantiles\n",
    "#### We thought an easy way to approach this would be to only use 4 labels, to do this we found the quantiles so we knew where to seperate the final grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numerical_df['G3'].quantile([0.25]))\n",
    "print(numerical_df['G3'].quantile([0.50]))\n",
    "print(numerical_df['G3'].quantile([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a copy of the data frame to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df2=numerical_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the Data\n",
    "#### Getting rid of first and second semester grades because we don't need to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df=numerical_df.drop(columns=['G1','G2'])\n",
    "numerical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_accuracy(df3,N):\n",
    "    \"\"\"\n",
    "    This function takes in an already cleaned dataframe and tests how accurately\n",
    "    it can predict students that fall into N quantiles\n",
    "        Inputs: df, a dataframe. N, an interger.\n",
    "        Outputs: The accuracy score.\n",
    "    \"\"\"\n",
    "    df2 = df3.sort_values(by = ['G3'])\n",
    "    df = df2['G3']\n",
    "\n",
    "    L = df.size\n",
    "    step = L/N\n",
    "    for i in range(N):\n",
    "        df.iloc[round((i)*step):round((i+1)*step)] = i\n",
    "    \n",
    "    \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(df2.drop(columns=['G3']),df)\n",
    "    model = svm.SVC(C=10,kernel='linear')\n",
    "    \n",
    "    model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "    predict=model_fit.predict(test_features)\n",
    "    return accuracy_score(predict, test_labels.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(2,10,1):\n",
    "    y.append(quantile_accuracy(numerical_df,i))\n",
    "plt.plot(range(2,10,1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=train_test_split(numerical_df.drop(columns=['G3']),numerical_df['G3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=10,kernel='linear')\n",
    "model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "predict=model_fit.predict(test_features)\n",
    "print(accuracy_score(predict, test_labels.astype(int)))\n",
    "print(confusion_matrix(test_labels.astype(int), predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "### The previous attempt was too easy, we're going to try to calculate final percentage instead of just quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the Data\n",
    "#### Getting rid of first and second semester grades because we don't need to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df2=numerical_df2.drop(columns=['G1','G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplying by 100 to turn the values into whole nubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df2['G3']=numerical_df2['G3']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=train_test_split(numerical_df2.drop(columns=['G3']),numerical_df2['G3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model\n",
    "#### This model is obviously not as good as our previous trial, but it still works.  If this was totally randomized the accuracey score would be 5.5%, so our method does have some predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=10,kernel='linear')\n",
    "model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "predict=model_fit.predict(test_features)\n",
    "print(accuracy_score(predict, test_labels.astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3\n",
    "### We want to try and improve on trial 2 while still using final percentages as labels.  In this trial we will only use features with low p values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features with p values under 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=numerical_df2[['G3','failures','famsize','schoolsup','romantic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=train_test_split(new_df.drop(columns=['G3']),new_df['G3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model\n",
    "#### The results of this trial were very similar to trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=10,kernel='linear')\n",
    "model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "predict=model_fit.predict(test_features)\n",
    "print(accuracy_score(predict, test_labels.astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another test with features that have a coorelation of over 0.1 with G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=numerical_df2[[\"G3\",\"sex\", \"age\", \"address\", \"Medu\", \"Fedu\", \"Mjob\", \"traveltime\", \"failures\", \"paid\", \"higher\", \"romantic\", \"goout\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=train_test_split(newdf.drop(columns=['G3']),newdf['G3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results were similar to trial 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=10,kernel='linear')\n",
    "model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "predict=model_fit.predict(test_features)\n",
    "print(accuracy_score(predict, test_labels.astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4\n",
    "### We want to retry trial 3, but with better parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using param_grid to find the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "# make a classifier by searching over a classifier and the parameter grid\n",
    "clf = GridSearchCV(svm.SVC(kernel='linear', class_weight='balanced'), param_grid)\n",
    "\n",
    "# we have a \"good\" classifier (according to GridSearchCV), how's it look\n",
    "clf = clf.fit(train_features, train_labels.astype(int))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=train_test_split(numerical_df2.drop(columns=['G3']),numerical_df2['G3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Model\n",
    "#### The results were still similar to trial 2 &3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=1000,kernel='linear',gamma=0.0001,class_weight='balanced')\n",
    "model_fit=model.fit(train_features,train_labels.astype(int))\n",
    "predict=model_fit.predict(test_features)\n",
    "print(accuracy_score(predict, test_labels.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
